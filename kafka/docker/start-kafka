#!/bin/bash

# If LOG_TO_STDOUT is defined in the environment, tee the output so that it is also logged to stdout.
# This is generally desirable in a development setup where you want to see the output on the console when
# starting a component, but not in production where we do not want logging to be captured by Docker
# and consume disk space (Docker JSON log driver captures and saves them then docker logs shows them).
# In a production environment, get the logs from the rsyslog component instead.
if [[ -z ${LOG_TO_STDOUT} ]]; then
  export LOGGER_COMMAND="logger --tag kafka-${BROKER_ID} -u /tmp/log.sock"
else
  export LOGGER_COMMAND="eval tee >(logger --tag kafka-${BROKER_ID} -u /tmp/log.sock)"
fi

start_kafka() {
    kafka_bin=/opt/kafka/bin/kafka-server-start.sh

    kafka_server_config=/opt/kafka/config/server.properties

    #export KAFKA_OPTS="-Djava.net.preferIPv4Stack=true"

    env

    if [ -n "$CURRENT_KAFKA_VERSION" ]; then
        KAFKA_PARAMS="$KAFKA_PARAMS --override inter.broker.protocol.version=$CURRENT_KAFKA_VERSION"
    fi

    if [ -n "$CURRENT_MESSAGE_FORMAT_VERSION" ]; then
        KAFKA_PARAMS="$KAFKA_PARAMS --override log.message.format.version=$CURRENT_MESSAGE_FORMAT_VERSION"
    fi

    KAFKA_PARAMS="$KAFKA_PARAMS --override broker.id=$BROKER_ID"
    KAFKA_PARAMS="$KAFKA_PARAMS --override zookeeper.connect=$ZOOKEEPER_HOSTS"
    # set up configuration for an INTERNAL listener and an EXTERNAL listener. clients connecting on the
    # INTERNAL port will be mapped to internally-resolvable (i.e. inside the docker network) broker
    # addresses, while clients connecting to the EXTERNAL port will be mapped to externally (outside
    # the docker network) accessible broker addresses.
    # For more info, see
    #   https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
    KAFKA_PARAMS="$KAFKA_PARAMS --override inter.broker.listener.name=INTERNAL"
    KAFKA_PARAMS="$KAFKA_PARAMS --override listener.security.protocol.map=PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
    if [ -n "$KAFKA_EXTERNAL_BROKER_ADDRESS" ] && [ -n "$KAFKA_EXTERNAL_PORT" ]; then
        # add configuration for supporting both internal and external access
        KAFKA_PARAMS="$KAFKA_PARAMS --override listeners=INTERNAL://0.0.0.0:$KAFKA_INTERNAL_PORT,EXTERNAL://0.0.0.0:$KAFKA_EXTERNAL_PORT"
        KAFKA_PARAMS="$KAFKA_PARAMS --override advertised.listeners=INTERNAL://$KAFKA_INTERNAL_BROKER_ADDRESS:$KAFKA_INTERNAL_PORT,EXTERNAL://$KAFKA_EXTERNAL_BROKER_ADDRESS:$KAFKA_EXTERNAL_PORT"
    else
        # default: only listen internally
        # KAFKA_PARAMS="$KAFKA_PARAMS --override listeners=PLAINTEXT://:$KAFKA_INTERNAL_PORT"
        KAFKA_PARAMS="$KAFKA_PARAMS --override listeners=INTERNAL://:$KAFKA_INTERNAL_PORT"
    fi
    # Set a default retention period to 24 hours for now. TODO: refine this setting per-topic based
    # our topic-specific needs.
    KAFKA_PARAMS="$KAFKA_PARAMS --override log.retention.hours=$KAFKA_LOG_RETENTION_HRS"
    # set the max.message.bytes for the whole server, until we can tune this with more granularity
    KAFKA_PARAMS="$KAFKA_PARAMS --override message.max.bytes=$KAFKA_MAX_MESSAGE_BYTES"

    echo Executing kafka with options: $KAFKA_PARAMS

    # disable GC logging in order, as we do not have any space to write it.
    export KAFKA_GC_LOG_OPTS=" "

    $kafka_bin $kafka_server_config $KAFKA_PARAMS
}

/usr/sbin/rsyslogd -f /etc/rsyslog.conf -i /tmp/rsyslog.pid

start_kafka | $LOGGER_COMMAND
