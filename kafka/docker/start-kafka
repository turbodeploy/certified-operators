#!/bin/bash

# If LOG_TO_STDOUT is defined in the environment, tee the output so that it is also logged to stdout.
# This is generally desirable in a development setup where you want to see the output on the console when
# starting a component, but not in production where we do not want logging to be captured by Docker
# and consume disk space (Docker JSON log driver captures and saves them then docker logs shows them).
# In a production environment, get the logs from the rsyslog component instead.
. /etc/kafka.profile

if [[ -z ${LOG_TO_STDOUT} ]]; then
  export LOGGER_COMMAND="logger --tag kafka-${BROKER_ID} -u /tmp/log.sock"
else
  export LOGGER_COMMAND="eval tee >(logger --tag kafka-${BROKER_ID} -u /tmp/log.sock)"
fi

kafka_bin=/opt/kafka/bin/kafka-server-start.sh

/usr/sbin/rsyslogd -f /etc/rsyslog.conf -i /tmp/rsyslog.pid

if [ -n "$CURRENT_KAFKA_VERSION" ]; then
    KAFKA_PARAMS="$KAFKA_PARAMS --override inter.broker.protocol.version=$CURRENT_KAFKA_VERSION"
fi

if [ -n "$CURRENT_MESSAGE_FORMAT_VERSION" ]; then
    KAFKA_PARAMS="$KAFKA_PARAMS --override log.message.format.version=$CURRENT_MESSAGE_FORMAT_VERSION"
fi

KAFKA_PARAMS="$KAFKA_PARAMS --override broker.id=$BROKER_ID"
KAFKA_PARAMS="$KAFKA_PARAMS --override zookeeper.connect=$ZOOKEEPER_HOSTS"
KAFKA_PARAMS="$KAFKA_PARAMS --override log.dirs=$KAFKA_DATA_DIR"
# set up configuration for an INTERNAL listener and an EXTERNAL listener. clients connecting on the
# INTERNAL port will be mapped to internally-resolvable (i.e. inside the docker network) broker
# addresses, while clients connecting to the EXTERNAL port will be mapped to externally (outside
# the docker network) accessible broker addresses.
# For more info, see
#   https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
KAFKA_PARAMS="$KAFKA_PARAMS --override inter.broker.listener.name=INTERNAL"
KAFKA_PARAMS="$KAFKA_PARAMS --override listener.security.protocol.map=PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
if [ -n "$KAFKA_EXTERNAL_BROKER_ADDRESS" ] && [ -n "$KAFKA_EXTERNAL_PORT" ]; then
    # add configuration for supporting both internal and external access
    KAFKA_PARAMS="$KAFKA_PARAMS --override listeners=INTERNAL://0.0.0.0:$KAFKA_INTERNAL_PORT,EXTERNAL://0.0.0.0:$KAFKA_EXTERNAL_PORT"
    KAFKA_PARAMS="$KAFKA_PARAMS --override advertised.listeners=INTERNAL://$KAFKA_INTERNAL_BROKER_ADDRESS:$KAFKA_INTERNAL_PORT,EXTERNAL://$KAFKA_EXTERNAL_BROKER_ADDRESS:$KAFKA_EXTERNAL_PORT"
else
    # default: only listen internally
    # KAFKA_PARAMS="$KAFKA_PARAMS --override listeners=PLAINTEXT://:$KAFKA_INTERNAL_PORT"
    KAFKA_PARAMS="$KAFKA_PARAMS --override listeners=INTERNAL://:$KAFKA_INTERNAL_PORT"
fi

if [ -z "$KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" ]; then
    # we'll default zookeeper timeout to 18 seconds now, which is also the new default in kafka 2.5.0+
    KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS="18000"
fi

# Set a default retention period to 24 hours for now. TODO: refine this setting per-topic based
# our topic-specific needs.
KAFKA_PARAMS="$KAFKA_PARAMS --override log.retention.hours=$KAFKA_LOG_RETENTION_HRS"
# set the max.message.bytes for the whole server, until we can tune this with more granularity
KAFKA_PARAMS="$KAFKA_PARAMS --override message.max.bytes=$KAFKA_MAX_MESSAGE_BYTES"
# set the broker default message timestamp type
KAFKA_PARAMS="$KAFKA_PARAMS --override log.message.timestamp.type=LogAppendTime"
# override the zookeeper session timeout. This should help us with noisy / resource-congested environments.
KAFKA_PARAMS="$KAFKA_PARAMS --override zookeeper.session.timeout.ms=$KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS"

echo Executing kafka with options: $KAFKA_PARAMS | $LOGGER_COMMAND

# disable GC logging in order, as we do not have any space to write it.
export KAFKA_GC_LOG_OPTS=" "

# move out the old data directory if exist
if [ -d /home/kafka/data/data ]; then
  mv /home/kafka/data/data /home/kafka/data.old
fi

exec $kafka_bin $KAFKA_SERVER_CONFIG $KAFKA_PARAMS > >($LOGGER_COMMAND) 2>&1

